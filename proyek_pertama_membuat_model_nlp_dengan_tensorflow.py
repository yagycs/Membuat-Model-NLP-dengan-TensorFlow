# -*- coding: utf-8 -*-
"""Proyek Pertama : Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wWBJRleQL4OIIFiSc107EaWvCnCO4w5j

Belajar Pengembangan Machine Learning<br>
Proyek Pertama : Membuat Model NLP dengan TensorFlow<br>
Nama  : Yagy Christoper Sumule<br>
Email : yagybatman@gmail.com

Kriteria: 
* Dataset yang akan dipakai bebas, namun minimal memiliki 1000 sampel.
* Harus menggunakan LSTM dalam arsitektur model.
* Harus menggunakan model sequential.
* Validation set sebesar 20% dari total dataset.
* Harus menggunakan Embedding.
* Harus menggunakan fungsi tokenizer.
* Akurasi dari model minimal 75% pada train set dan validation set.
---
Anda dapat menerapkan beberapa saran untuk mendapatkan nilai tinggi, berikut sarannya:
* Akurasi dari model di atas 80%.
* Mengimplementasikan callback.
* Membuat plot loss dan akurasi pada saat training dan validation.
---
Detail penilaian submission:
* Bintang 5 : Semua ketentuan terpenuhi, dataset memiliki 3 kelas atau lebih dan minimal 2000 sampel data. Serta akurasi pada training set dan validation set di atas 90%.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow as tf
import matplotlib.pyplot as plt
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')

df = pd.read_csv('subjects-questions.csv')

df.head()

df.isnull().values.any()

# case folding
def clean_lower(lwr):
  lwr = lwr.lower() # lowercase text
  return lwr

# Buat kolom tambahan untuk data description yang telah dicasefolding  
df['lwr'] = df['eng'].apply(clean_lower)
casefolding=pd.DataFrame(df['lwr'])
casefolding

#Remove Puncutuation
clean_spcl = re.compile('[/(){}\[\]\|@,;]')
clean_symbol = re.compile('[^0-9a-z]')
def clean_punct(text):
    text = clean_spcl.sub('', text)
    text = clean_symbol.sub(' ', text)
    return text

# Buat kolom tambahan untuk data description yang telah diremovepunctuation   
df['clean_punct'] = df['lwr'].apply(clean_punct)
df['clean_punct']

#clean stopwords
stopword = set(stopwords.words('english'))

def clean_stopwords(text):
    text = ' '.join(word for word in text.split() if word not in stopword) # hapus stopword dari kolom deskripsi
    return text

# Buat kolom tambahan untuk data description yang telah distopwordsremoval   
df['clean_sw'] = df['clean_punct'].apply(clean_stopwords)

add = pd.DataFrame(df['clean_sw'])
df['add_swr']= add.replace(to_replace =['whether','yes','also','thanks','take','whatever',
                    'making','makes','taking','takes','ok','oh','etc',
                                        "yep"],  
                            value ="", regex= True) 
df['add_swr']

df.head()

course = pd.get_dummies(df.Subject)
df_baru = pd.concat([df, course], axis=1)
df_baru = df_baru.drop(columns='Subject')
df_baru = df_baru.drop(columns='eng')
df_baru = df_baru.drop(columns='lwr')
df_baru = df_baru.drop(columns='clean_punct')
df_baru = df_baru.drop(columns='clean_sw')
df_baru.head()

questions= df_baru['add_swr'].values
label = df_baru[['Biology', 'Chemistry', 'Maths', 'Physics']].values

questions_latih, questions_test, label_latih, label_test = train_test_split(questions, label, test_size=0.2)

tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(questions_latih)
tokenizer.fit_on_texts(questions_test)

sekuens_latih = tokenizer.texts_to_sequences(questions_latih)
sekuens_test = tokenizer.texts_to_sequences(questions_test)

padded_latih = pad_sequences(sekuens_latih,
                             padding='post',
                             maxlen=200,
                             truncating='post')
padded_test = pad_sequences(sekuens_test,
                            padding='post',
                            maxlen=200,
                            truncating='post')

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
    tf.keras.layers.LSTM(64),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

class Callback(tf.keras.callbacks.Callback): 
    def on_epoch_end(self, epoch, logs={}): 
        if(logs.get('accuracy') > 0.90 and logs.get('val_accuracy') > 0.90):
            print("\nAkurasi > 90% ") 
            self.model.stop_training = True 
     
callbacks = Callback()

num_epochs = 30
history = model.fit(padded_latih, label_latih, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], ['Test'], loc='upper left')
plt.show

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], ['Test'], loc='upper left')
plt.show